import cv2
import torch
import transformers
import speech_recognition as sr
from gtts import gTTS
import os

# Load YOLOv5 model from ultralytics
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Initialize video capture from EpocCam (usually detected as the first webcam)
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("Error: Could not open video stream from EpocCam")
    exit()

# Initialize speech recognition
recognizer = sr.Recognizer()

# Load the Hugging Face question-answering model
qa_pipeline = transformers.pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

def answer_query(query, detected_objects):
    objects_description = ". ".join([f"There is a {obj[0]} with confidence {obj[1]:.2f}." for obj in detected_objects])
    context = f"The objects detected in the current frame are: {objects_description}"
    
    response = qa_pipeline(question=query, context=context)
    
    return response['answer']

def listen_for_query():
    with sr.Microphone() as source:
        print("Listening for query...")
        audio = recognizer.listen(source)
        try:
            query = recognizer.recognize_google(audio)
            print(f"Query: {query}")
            return query
        except sr.UnknownValueError:
            print("Sorry, I could not understand the audio.")
            return None
        except sr.RequestError:
            print("Could not request results from Google Speech Recognition service.")
            return None

def speak_answer(answer):
    print(f"Answer: {answer}")
    tts = gTTS(text=answer, lang='en')
    tts.save("answer.mp3")
    os.system("mpg321 answer.mp3")  # You may need to install mpg321 or change the command to suit your OS

detected_objects = []

while True:
    ret, frame = cap.read()
    if not ret:
        print("Error: Failed to capture image")
        break

    # Convert frame to RGB
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Run detection
    results = model(img_rgb)

    # Extract results
    labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]

    n = len(labels)
    x_shape, y_shape = frame.shape[1], frame.shape[0]

    detected_objects = []  # Clear previous detections
    for i in range(n):
        row = cord[i]
        if row[4] >= 0.2:  # confidence threshold
            x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)
            bgr = (0, 255, 0)
            cv2.rectangle(frame, (x1, y1), (x2, y2), bgr, 2)
            label = model.names[int(labels[i])]
            confidence = row[4]
            cv2.putText(frame, f"{label} {confidence:.2f}",
                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2)
            detected_objects.append((label, confidence, (x1, y1, x2, y2)))

    # Display the resulting frame
    cv2.imshow('YOLOv5 Detection', frame)

    # Check for user input to process query
    if cv2.waitKey(1) & 0xFF == ord('q'):
        query = listen_for_query()
        if query:
            answer = answer_query(query, detected_objects)
            speak_answer(answer)
        break

cap.release()
cv2.destroyAllWindows()
