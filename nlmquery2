import cv2
import torch
import transformers
import sounddevice as sd
import numpy as np
from gtts import gTTS
import os
import speech_recognition as sr

# Load YOLOv5 model from ultralytics
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Initialize video capture from EpocCam (usually detected as the first webcam)
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("Error: Could not open video stream from EpocCam")
    exit()

# Load the Hugging Face question-answering model
qa_pipeline = transformers.pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

def answer_query(query, detected_objects):
    objects_description = ". ".join([f"There is a {obj[0]} with confidence {obj[1]:.2f}." for obj in detected_objects])
    context = f"The objects detected in the current frame are: {objects_description}"
    
    # Extend context to include common objects held by people
    common_objects = ["phone", "book", "bottle", "cup", "bag"]
    context += f" Common objects people might hold include: {', '.join(common_objects)}"
    
    response = qa_pipeline(question=query, context=context)
    
    return response['answer']

def listen_for_query():
    r = sr.Recognizer()
    with sr.Microphone() as source:
        print("Listening for query...")
        audio_data = r.listen(source)
        print("Recognizing...")
        query = r.recognize_google(audio_data)
        print(f"Query: {query}")
        return query

def speak_answer(answer):
    print(f"Answer: {answer}")
    tts = gTTS(text=answer, lang='en')
    tts.save("answer.mp3")
    os.system("mpg321 answer.mp3")  # You may need to install mpg321 or change the command to suit your OS

detected_objects = []

while True:
    ret, frame = cap.read()
    if not ret:
        print("Error: Failed to capture image")
        break

    # Convert frame to RGB
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Run detection
    results = model(img_rgb)

    # Extract results
    labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]

    n = len(labels)
    x_shape, y_shape = frame.shape[1], frame.shape[0]

    detected_objects = []  # Clear previous detections
    for i in range(n):
        row = cord[i]
        if row[4] >= 0.2:  # confidence threshold
            x1, y1, x2, y2 = int(row[0]*x_shape), int(row[1]*y_shape), int(row[2]*x_shape), int(row[3]*y_shape)
            bgr = (0, 255, 0)
            cv2.rectangle(frame, (x1, y1), (x2, y2), bgr, 2)
            label = model.names[int(labels[i])]
            confidence = row[4]
            cv2.putText(frame, f"{label} {confidence:.2f}",
                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, bgr, 2)
            detected_objects.append((label, confidence, (x1, y1, x2, y2)))

    # Display the resulting frame
    cv2.imshow('YOLOv5 Detection', frame)

    # Check for user input to process query
    if cv2.waitKey(1) & 0xFF == ord('q'):
        query = listen_for_query()
        if query:  # Ensure query is not empty
            answer = answer_query(query, detected_objects)
            speak_answer(answer)
        break

cap.release()
cv2.destroyAllWindows()
